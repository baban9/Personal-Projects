{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build an LSTM model to learn sequences of words from email data. We will use this model to predict the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the next word based on the sequence of words or sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_content = pd.read_csv(r'C:\\Users\\baban\\Python Code\\Python\\MINI PROJECT\\NLP\\spam.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just selecting emails and connverting it into list\n",
    "Email_Data = file_content[[ 'v2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data = Email_Data.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import collections\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "tokenizer = ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting list to string\n",
    "from collections import Iterable\n",
    "def flatten(items):\n",
    "    \"\"\"Yield items from any nested iterable\"\"\"\n",
    "    for x in items:\n",
    "        if isinstance(x, Iterable) and not isinstance(x,(str, bytes)):\n",
    "            for sub_x in flatten(x):\n",
    "                yield sub_x\n",
    "        else:\n",
    "                yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextData=list(flatten(list_data))\n",
    "TextData =''.join(TextData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted lines and converting into lower case\n",
    "TextData = TextData.replace('\\n','')\n",
    "TextData = TextData.lower()\n",
    "\n",
    "pattern = r'[^a-zA-z0-9\\s]'\n",
    "TextData = re.sub(pattern, '', ''.join(TextData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing\n",
    "tokens = tokenizer.tokenize(TextData)\n",
    "tokens = [token.strip() for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "13345\n"
     ]
    }
   ],
   "source": [
    "# get the distinct words and sort it\n",
    "word_counts = collections.Counter(tokens)\n",
    "word_c = len(word_counts)\n",
    "\n",
    "print(word_c)\n",
    "distinct_words = [x[0] for x in word_counts.most_common()]\n",
    "distinct_words_sorted = list(sorted(distinct_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate indexing for all words\n",
    "word_index = {x: i for i, x in enumerate(distinct_words_sorted)}\n",
    "# decide on sentence length\n",
    "sentence_length = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "# Generate the data for the model\n",
    "\n",
    "\n",
    "# input = the input sentence to the model with index\n",
    "# output = output of the model with index\n",
    "\n",
    "InputData = []\n",
    "OutputData = []\n",
    "for i in range(0, word_c - sentence_length, 1):\n",
    "    X = tokens[i:i + sentence_length]\n",
    "    Y = tokens[i + sentence_length]\n",
    "    InputData.append([word_index[char] for char in X])\n",
    "    OutputData.append(word_index[Y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[5086, 12190, 6352, 9096, 3352, 1920, 8507, 5937, 2535, 7886, 5214, 12910, 6541, 4104, 2531, 2997, 11473, 5170, 1595, 12552, 6590, 6316, 12758, 12087, 8496]]\n\n\n[4292]\n"
     ]
    }
   ],
   "source": [
    "print (InputData[:1])\n",
    "print (\"\\n\")\n",
    "print(OutputData[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate X\n",
    "X = numpy.reshape(InputData, (len(InputData), sentence_length, 1))\n",
    "# One hot encode the output variable\n",
    "Y = np_utils.to_categorical(OutputData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the checkpoint\n",
    "file_name_path=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_name_path, monitor='loss',verbose=1, save_best_only=True, mode='min')\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\baban\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/15\n",
      "13320/13320 [==============================] - 39s 3ms/step - loss: 7.8639\n",
      "\n",
      "Epoch 00001: loss improved from inf to 7.86393, saving model to weights-improvement-01-7.8639.hdf5\n",
      "Epoch 2/15\n",
      "13320/13320 [==============================] - 31s 2ms/step - loss: 7.1548\n",
      "\n",
      "Epoch 00002: loss improved from 7.86393 to 7.15483, saving model to weights-improvement-02-7.1548.hdf5\n",
      "Epoch 3/15\n",
      "13320/13320 [==============================] - 34s 3ms/step - loss: 7.0207\n",
      "\n",
      "Epoch 00003: loss improved from 7.15483 to 7.02071, saving model to weights-improvement-03-7.0207.hdf5\n",
      "Epoch 4/15\n",
      "13320/13320 [==============================] - 33s 2ms/step - loss: 6.8878\n",
      "\n",
      "Epoch 00004: loss improved from 7.02071 to 6.88779, saving model to weights-improvement-04-6.8878.hdf5\n",
      "Epoch 5/15\n",
      "13320/13320 [==============================] - 31s 2ms/step - loss: 6.7928\n",
      "\n",
      "Epoch 00005: loss improved from 6.88779 to 6.79284, saving model to weights-improvement-05-6.7928.hdf5\n",
      "Epoch 6/15\n",
      "13320/13320 [==============================] - 41s 3ms/step - loss: 6.7147\n",
      "\n",
      "Epoch 00006: loss improved from 6.79284 to 6.71465, saving model to weights-improvement-06-6.7147.hdf5\n",
      "Epoch 7/15\n",
      "13320/13320 [==============================] - 37s 3ms/step - loss: 6.6802\n",
      "\n",
      "Epoch 00007: loss improved from 6.71465 to 6.68021, saving model to weights-improvement-07-6.6802.hdf5\n",
      "Epoch 8/15\n",
      "13320/13320 [==============================] - 34s 3ms/step - loss: 6.6310\n",
      "\n",
      "Epoch 00008: loss improved from 6.68021 to 6.63098, saving model to weights-improvement-08-6.6310.hdf5\n",
      "Epoch 9/15\n",
      "13320/13320 [==============================] - 34s 3ms/step - loss: 6.5995\n",
      "\n",
      "Epoch 00009: loss improved from 6.63098 to 6.59951, saving model to weights-improvement-09-6.5995.hdf5\n",
      "Epoch 10/15\n",
      "13320/13320 [==============================] - 30s 2ms/step - loss: 6.5633\n",
      "\n",
      "Epoch 00010: loss improved from 6.59951 to 6.56329, saving model to weights-improvement-10-6.5633.hdf5\n",
      "Epoch 11/15\n",
      "13320/13320 [==============================] - 36s 3ms/step - loss: 6.5880\n",
      "\n",
      "Epoch 00011: loss did not improve from 6.56329\n",
      "Epoch 12/15\n",
      "13320/13320 [==============================] - 44s 3ms/step - loss: 6.5598\n",
      "\n",
      "Epoch 00012: loss improved from 6.56329 to 6.55980, saving model to weights-improvement-12-6.5598.hdf5\n",
      "Epoch 13/15\n",
      "13320/13320 [==============================] - 37s 3ms/step - loss: 6.5853\n",
      "\n",
      "Epoch 00013: loss did not improve from 6.55980\n",
      "Epoch 14/15\n",
      "13320/13320 [==============================] - 46s 3ms/step - loss: 6.5336\n",
      "\n",
      "Epoch 00014: loss improved from 6.55980 to 6.53360, saving model to weights-improvement-14-6.5336.hdf5\n",
      "Epoch 15/15\n",
      "13320/13320 [==============================] - 47s 4ms/step - loss: 6.5199\n",
      "\n",
      "Epoch 00015: loss improved from 6.53360 to 6.51990, saving model to weights-improvement-15-6.5199.hdf5\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x22cf5193548>"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "#fit the model\n",
    "model.fit(X, Y, epochs=15, batch_size=128, callbacks=callbacks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "try:\n",
    "    file_name = \"weights-improvement-15-6.5199.hdf5\"\n",
    "    model.load_weights(file_name)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "except OSError:\n",
    "    print(\" You forgot to get the updated file. Look for .hdf5  extension file in your working directory\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating random sequence\n",
    "start = numpy.random.randint(0, len(InputData))\n",
    "input_sent = InputData[start]\n",
    "# Generate index of the next word of the email\n",
    "X = numpy.reshape(input_sent, (1, len(input_sent), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\baban\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "[667, 8859, 9106, 12187, 184, 7781, 3604, 5403, 1221, 8018, 3581, 6105, 5696, 13207, 4267, 13273, 4953, 13171, 10692, 6213, 8153, 2132, 5937, 11911, 2571]\n",
      "\n",
      "\n",
      "5849\n"
     ]
    }
   ],
   "source": [
    "predict_word = model.predict(X, verbose=0)\n",
    "index = numpy.argmax(predict_word)\n",
    "print(input_sent)\n",
    "print (\"\\n\")\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert these indexes back to words\n",
    "word_index_rev = dict((i, c) for i, c in enumerate(tokens))\n",
    "result = word_index_rev[index]\n",
    "sent_in = [word_index_rev[value] for value in input_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "let okits or me colour afternoon same be me that msg busetopmessagesome to are thats verify with have from minutes sunscreen the would credits new \n------ Next word is ------\nshut\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(sent_in),\"\\n------ Next word is ------\\n{}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\baban\\\\Personal-Projects'"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}