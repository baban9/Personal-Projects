# -*- coding: utf-8 -*-
"""Q2-Seq2Seq for Translation

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hOfcvRqSBoH9bCnSACbGW0fvhhnRP-gb


*   Babandeep Singh
*   Robin Beura
*   Rahul Gera
*   Sanghamitra Muhuri
*   Yash Patel
"""

import os
from google.colab import drive
drive.mount('/content/drive')
os.chdir('/content/drive/Shared drives/IDS 576/Assignment 3')

dir = os.getcwd()

# pip install pytorch-nlp
from io import open
import unicodedata
import string
import re
import random

import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""####Data Loading """

# Representing words as vectors
SOS_token = 0
EOS_token = 1

"""using class - Lang which has word → index (word2index) and index → word (index2word) dictionaries
and count of each word word2count to use to later replace rare words.
"""
class Lang:
  
    def __init__(self, name):
        self.name = name
        self.word2index = {}
        self.word2count = {}
        self.index2word = {0: "SOS", 1: "EOS"}
        self.n_words = 2  # Count SOS and EOS

    def addSentence(self, sentence):
        for word in sentence.split(' '):
            self.addWord(word)

    def addWord(self, word):
        if word not in self.word2index:
            self.word2index[word] = self.n_words
            self.word2count[word] = 1
            self.index2word[self.n_words] = word
            self.n_words += 1
        else:
            self.word2count[word] += 1

def unicodeToAscii(s):
    return ''.join(
        c for c in unicodedata.normalize('NFD', s)
        if unicodedata.category(c) != 'Mn'
    )

# Lowercase, trim, and remove non-letter characters
def normalizeString(s):
    s = s.lower().strip()    # lowercase for consistency
    s = re.sub(r"([.!?])", r" \1", s)    # replacing punctuations with first matched group
    return s

"""Reading data from hin-eng.zip"""

def readLangs(lang1, lang2, reverse=False):
    print("Reading lines...")

    # Read the file and split into lines
    lines = open(dir+'/hin.txt', encoding='utf-8').read().strip().split('\n')

    # Split every line into pairs and normalize
    pairs = [[normalizeString(s) for s in l.split('\t')][:-1] for l in lines]

    # Reverse pairs, make Lang instances
    if reverse:
        pairs = [list(reversed(p)) for p in pairs]
        input_lang = Lang(lang2)
        output_lang = Lang(lang1)
    else:
        input_lang = Lang(lang1)
        output_lang = Lang(lang2)

    return input_lang, output_lang, pairs

# Taking only sentences less than 40 words
MAX_LENGTH = 40
 
def filterPair(p):
    return len(p[0].split(' ')) < MAX_LENGTH and \
        len(p[1].split(' ')) < MAX_LENGTH 
 
def filterPairs(pairs):
    return [pair for pair in pairs if filterPair(pair)]

def prepareData(lang1, lang2, reverse=False):
    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)
    print("Read %s sentence pairs" % len(pairs))
    pairs = filterPairs(pairs)
    print("Trimmed to %s sentence pairs" % len(pairs))
    print("Counting words...")
    for pair in pairs:
        input_lang.addSentence(pair[0])
        output_lang.addSentence(pair[1])
    print("Counted words:")
    print(input_lang.name, input_lang.n_words)
    print(output_lang.name, output_lang.n_words)
    return input_lang, output_lang, pairs

"""####Sequence to Sequence Modelling:"""

class EncoderRNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(EncoderRNN, self).__init__()
        self.hidden_size = hidden_size

        self.embedding = nn.Embedding(input_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size)

    def forward(self, input, hidden):
        embedded = self.embedding(input).view(1, 1, -1)
        output = embedded
        output, hidden = self.gru(output, hidden)
        return output, hidden

    def initHidden(self):
        return torch.zeros(1, 1, self.hidden_size, device=device)

class AttnDecoderRNN(nn.Module):
    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):
        super(AttnDecoderRNN, self).__init__()
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.dropout_p = dropout_p
        self.max_length = max_length

        self.embedding = nn.Embedding(self.output_size, self.hidden_size)
        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)
        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)
        self.dropout = nn.Dropout(self.dropout_p)
        self.gru = nn.GRU(self.hidden_size, self.hidden_size)
        self.out = nn.Linear(self.hidden_size, self.output_size)

    def forward(self, input, hidden, encoder_outputs):
        embedded = self.embedding(input).view(1, 1, -1)
        embedded = self.dropout(embedded)

        attn_weights = F.softmax(
            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)
        attn_applied = torch.bmm(attn_weights.unsqueeze(0),
                                 encoder_outputs.unsqueeze(0))

        output = torch.cat((embedded[0], attn_applied[0]), 1)
        output = self.attn_combine(output).unsqueeze(0)

        output = F.relu(output)
        output, hidden = self.gru(output, hidden)

        output = F.log_softmax(self.out(output[0]), dim=1)
        return output, hidden, attn_weights

    def initHidden(self):
        return torch.zeros(1, 1, self.hidden_size, device=device)

"""Preparing Training Data"""

def indexesFromSentence(lang, sentence):
    return [lang.word2index[word] for word in sentence.split(' ')]


def tensorFromSentence(lang, sentence):
    indexes = indexesFromSentence(lang, sentence)
    indexes.append(EOS_token)
    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)


def tensorsFromPair(pair):
    input_tensor = tensorFromSentence(input_lang, pair[0])
    target_tensor = tensorFromSentence(output_lang, pair[1])
    return (input_tensor, target_tensor)

"""Training the Model"""

# Commented out IPython magic to ensure Python compatibility.
teacher_forcing_ratio = 0.5

def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):
    encoder_hidden = encoder.initHidden()
#     %matplotlib inline

    encoder_optimizer.zero_grad()
    decoder_optimizer.zero_grad()

    input_length = input_tensor.size(0)
    target_length = target_tensor.size(0)

    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

    loss = 0

    for ei in range(input_length):
        encoder_output, encoder_hidden = encoder(
            input_tensor[ei], encoder_hidden)
        encoder_outputs[ei] = encoder_output[0, 0]

    decoder_input = torch.tensor([[SOS_token]], device=device)

    decoder_hidden = encoder_hidden

    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False

    if use_teacher_forcing:
        # Teacher forcing: Feed the target as the next input
        for di in range(target_length):
            decoder_output, decoder_hidden, decoder_attention = decoder(
                decoder_input, decoder_hidden, encoder_outputs)
            loss += criterion(decoder_output, target_tensor[di])
            decoder_input = target_tensor[di]  # Teacher forcing

    else:
        # Without teacher forcing: use its own predictions as the next input
        for di in range(target_length):
            decoder_output, decoder_hidden, decoder_attention = decoder(
                decoder_input, decoder_hidden, encoder_outputs)
            topv, topi = decoder_output.topk(1)
            decoder_input = topi.squeeze().detach()  # detach from history as input

            loss += criterion(decoder_output, target_tensor[di])
            if decoder_input.item() == EOS_token:
                break

    loss.backward()

    encoder_optimizer.step()
    decoder_optimizer.step()

    return loss.item() / target_length

"""Helper function to track time and progress"""

import time
import math


def asMinutes(s):
    m = math.floor(s / 60)
    s -= m * 60
    return '%dm %ds' % (m, s)


def timeSince(since, percent):
    now = time.time()
    s = now - since
    es = s / (percent)
    rs = es - s
    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))

def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):
    start = time.time()
    plot_losses = []
    print_loss_total = 0  # Reset every print_every
    plot_loss_total = 0  # Reset every plot_every

    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)
    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)
    training_pairs = [tensorsFromPair(random.choice(pairs))
                      for i in range(n_iters)]
    criterion = nn.NLLLoss()

    for iter in range(1, n_iters + 1):
        training_pair = training_pairs[iter - 1]
        input_tensor = training_pair[0]
        target_tensor = training_pair[1]

        loss = train(input_tensor, target_tensor, encoder,
                     decoder, encoder_optimizer, decoder_optimizer, criterion)
        print_loss_total += loss
        plot_loss_total += loss

        if iter % print_every == 0:
            print_loss_avg = print_loss_total / print_every
            print_loss_total = 0
            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),
                                         iter, iter / n_iters * 100, print_loss_avg))

        if iter % plot_every == 0:
            plot_loss_avg = plot_loss_total / plot_every
            plot_losses.append(plot_loss_avg)
            plot_loss_total = 0

    showPlot(plot_losses)

#Function for plotting results
import matplotlib.pyplot as plt
plt.switch_backend('agg')
import matplotlib.ticker as ticker
import warnings
warnings.filterwarnings("ignore", module="matplotlib")
import numpy as np

def showPlot(points):
    plt.figure()
    fig, ax = plt.subplots()
    # this locator puts ticks at regular intervals
    loc = ticker.MultipleLocator(base=0.2)
    ax.yaxis.set_major_locator(loc)
    plt.plot(points)

"""####Evaluation"""

def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):
    with torch.no_grad():
        input_tensor = tensorFromSentence(input_lang, sentence)
        input_length = input_tensor.size()[0]
        encoder_hidden = encoder.initHidden()

        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

        for ei in range(input_length):
            encoder_output, encoder_hidden = encoder(input_tensor[ei],
                                                     encoder_hidden)
            encoder_outputs[ei] += encoder_output[0, 0]

        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS

        decoder_hidden = encoder_hidden

        decoded_words = []
        decoder_attentions = torch.zeros(max_length, max_length)

        for di in range(max_length):
            decoder_output, decoder_hidden, decoder_attention = decoder(
                decoder_input, decoder_hidden, encoder_outputs)
            decoder_attentions[di] = decoder_attention.data
            topv, topi = decoder_output.data.topk(1)
            if topi.item() == EOS_token:
                decoded_words.append('')
                break
            else:
                decoded_words.append(output_lang.index2word[topi.item()])

            decoder_input = topi.squeeze().detach()

        return decoded_words, decoder_attentions[:di + 1]

def evaluateRandomly(encoder, decoder, n=10):
    from nltk.translate.bleu_score import sentence_bleu
    
    for i in range(n):
        pair = random.choice(pairs)
        print('Input: ', pair[0])
        print('Output: ', pair[1])
        output_words, attentions = evaluate(encoder, decoder, pair[0])
        output_sentence = ' '.join(output_words)
        print('Predicted: ', output_sentence)
        act = pair[1].split()
        pred = output_sentence.split()
        print('Cumulative 1-gram: %f' % sentence_bleu([act], pred, weights=(1, 0, 0, 0)))
        print('Cumulative 2-gram: %f' % sentence_bleu([act], pred, weights=(0.5, 0.5, 0, 0)))
        print('Cumulative 3-gram: %f' % sentence_bleu([act], pred, weights=(0.33, 0.33, 0.33, 0)))
        print('Cumulative 4-gram: %f' % sentence_bleu([act], pred, weights=(0.25, 0.25, 0.25, 0.25)))
        score = sentence_bleu([act], pred)   ##### using BLEU here
        print(score)
        print('')

"""Helper Functions end here

###Training and Evaluation

####**MODEL 1** Hindi to English
"""

# in case we want to reverse i/o pair
input_lang, output_lang, pairs = prepareData('eng', 'hin', True)  # True indicates Hindi to English translation
print(random.choice(pairs))

hidden_size = 256
encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)
attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)

trainIters(encoder1, attn_decoder1, 60000, print_every=2000)   #### 60000,2000

evaluateRandomly(encoder1, attn_decoder1)

"""Visualizing Attention output in matrix form with columns being input and rows being output steps:"""

output_words, attentions = evaluate(
    encoder1, attn_decoder1, "कैसा चल रहा है ?")
plt.matshow(attentions.numpy())

def showAttention(input_sentence, output_words, attentions):
    # Set up figure with colorbar
    fig = plt.figure()
    ax = fig.add_subplot(111)
    cax = ax.matshow(attentions.numpy(), cmap='bone')
    fig.colorbar(cax)

    # Set up axes
    ax.set_xticklabels([''] + input_sentence.split(' ') +
                       ['<EOS>'], rotation=90)
    ax.set_yticklabels([''] + output_words)

    # Show label at every tick
    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))
    
    plt.show()


def evaluateAndShowAttention(input_sentence):
    output_words, attentions = evaluate(
        encoder1, attn_decoder1, input_sentence)
    print('input =', input_sentence)
    print('output =', ' '.join(output_words))
    showAttention(input_sentence, output_words, attentions)

def evaluateAndShow1(input_sentence):
    output_words, attentions = evaluate(
        encoder1, attn_decoder1, input_sentence)
    print('input =', input_sentence)
    print('output =', ' '.join(output_words))

# Hindi to English test sentence
evaluateAndShowAttention("कैसा चल रहा है ?")

"""####**MODEL 2** English to Hindi

"""

input_lang, output_lang, pairs = prepareData('eng', 'hin', False)
print(random.choice(pairs))

"""#####**Vanilla Model**"""

hidden_size = 256
encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)
attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)

trainIters(encoder1, attn_decoder1, 60000, print_every=2000)  ### 60000,2000

evaluateRandomly(encoder1, attn_decoder1)

# English to Hindi test sentence
evaluateAndShowAttention("how are you ?")

"""#####Inputing sentences from English to Vanilla Model and input resultant translated sentences to Model 1. """

# Sentence 1: Model2 -> Hindi
evaluateAndShow1("how are you ?")

# Sentence 1: Model1 -> English
evaluateAndShow1("आप कैसे हो ?")

# Sentence 2: Model2 -> Hindi
evaluateAndShow1("i am waiting for you")

# Sentence 2: Model1 -> English
evaluateAndShow1("मैं तुम्हें कुछ बताने के लिए हूँ। ")

# Sentence 3: Model2 -> Hindi
evaluateAndShow1("it's cold outside")

# Sentence 3: Model1 -> English
evaluateAndShow1("क्या पता हो रहा है। ")

"""##### **Model 2 using GloVe Embeddings**

Loading GloVe 100 Dimemsional Embeddings:
"""

from torchtext.vocab import GloVe
from torchtext import data
TEXT = data.Field(tokenize='spacy')
TEXT.build_vocab(pairs,max_size=25000, vectors = GloVe(name='6B', dim=100))

input_lang, output_lang, pairs = prepareData('eng', 'hin', False)
print(random.choice(pairs))

"""Training Model using GloVe vectors"""

INPUT_DIM = len(TEXT.vocab) # updating new vocab
hidden_size = 100 # Glove dimension is 100
encoder2 = EncoderRNN(INPUT_DIM, hidden_size).to(device)
attn_decoder2 = AttnDecoderRNN(hidden_size, INPUT_DIM, dropout_p=0.1).to(device)

INPUT_DIM

pretrained_embeddings = TEXT.vocab.vectors
encoder2.embedding.weight.data.copy_(pretrained_embeddings)
attn_decoder2.embedding.weight.data.copy_(pretrained_embeddings)

trainIters(encoder2, attn_decoder2, 60000, print_every=2000)

"""Evaluating Performance for Model 2

####Inputing 5 sentences from English to Model2 and input resultant translated sentences to Model 1.
"""

def evaluateAndShow2(input_sentence):
    output_words, attentions = evaluate(
        encoder2, attn_decoder2, input_sentence)
    print('input =', input_sentence)
    print('output =', ' '.join(output_words))

# Sentence 1: Model2 -> Hindi
evaluateAndShow2("how are you ?")

# Sentence 1: Model1 -> English
evaluateAndShow1("आप कहाँ हो ?")

# Sentence 2: Model2 -> Hindi
evaluateAndShow2("i am waiting for you")

# Sentence 2: Model1 -> English
evaluateAndShow1("मैं क्या मैं सकता हूँ ? ")

# Sentence 3: Model2 -> Hindi
evaluateAndShow2("it's cold outside")

# Sentence 3: Model1 -> English
evaluateAndShow1("मुश्किल क्या !")

# Sentence 4: Model2 -> Hindi
evaluateAndShow2("we are playing today")

# Sentence 4: Model1 -> English
evaluateAndShow1("हम कल से यकीन नहीं ")

# Sentence 5: Model2 -> Hindi
evaluateAndShow2("let's finish this")

# Sentence 5: Model1 -> English
evaluateAndShow1("मुश्किल क्या हो !")

"""In conclusion,  the Vanilla model gives better results when compared to that with GloVe embeddings for the Hindi language translation. Modified GloVe embeddings would perform better when used for Indian Languages. This is because Hindi vocabulary which we used here had relatively fewer words as compared to other languages. If we increase the vocabulary or use modified embeddings, we'll achieve better results.

(Refer: https://www.aclweb.org/anthology/2020.sltu-1.49.pdf)

##References:

1. https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html
"""
